{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51318bfb",
   "metadata": {},
   "source": [
    "NB : le fichier algorithm.py contient de nombreuses lignes de codes passées en commentaires, notamment à la fin du code, permettant le plus souvent d'afficher des valeurs calculées au cours de l'algorithme ou de plot des graphiques adéquats. Le lecteur peut donc aisément décommenter ces lignes pour pouvoir afficher ce qu'il souhaite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9f34e",
   "metadata": {},
   "source": [
    "# 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df827030",
   "metadata": {},
   "source": [
    "On a ici changé la taille de fenêtre pour l'échantillon ainsi que l'overlap, les valeurs proposées dans l'énoncé ne donnant souvent pas de résultats satisfaisants.\n",
    "\n",
    "Les lignes de code pour ne garder que 90% de l'énergie du signal sont expliquées dans le fichier algorithm.py, l'énergie n'étant pas du tout répartie de manière homogène, puisque l'on passe en moyenne de l'ordre de 10^7 à seulement 10^4 coefficients après filtrage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332cb86",
   "metadata": {},
   "source": [
    "# 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75f38c",
   "metadata": {},
   "source": [
    "On détermine ici empiriquement le coefficient fonctionnant le mieux pour la min_distance, et on rajoute un argument num_peaks restreignant le nombre total de pics renvoyé, pour le rendre proportionnel à la longueur de l'échantillon rentré (un morceau entier aura donc mécaniquement beaucoup plus de pics qu'un échantillon)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b04d01",
   "metadata": {},
   "source": [
    "# 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb84dca",
   "metadata": {},
   "source": [
    "Le choix des delta f et delta t fonctionnant le mieux pour le code de hashage fut compliqué, car plus ils seront grands et meilleure sera l'identification en moyenne, car on augmente beaucoup la taille du code de hash, mais en contrepartie l'algorithme mettra beaucoup plus de temps à comparer 2 échantillons.\n",
    "\n",
    "Les valeurs ici choisies ne sont donc sans doute pas encore du tout optimales, mais fonctionnent au moins en un temps raisonnable.\n",
    "\n",
    "Le code de hash est donc invariant par translation, la valeur du temps de début n'est pas importante, car on regarde à chaque fois les points autour des ancres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb46825",
   "metadata": {},
   "source": [
    "# 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31069cd",
   "metadata": {},
   "source": [
    "Lorsque les morceaux testé coïncident, on observe dans la fonction display_scatterplot une droite se tracer au milieu du nuage de point ambiant, alors que s'ils ne coïncident pas le nuage de point est complètement aléatoirement réparti et rien n'est notable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ae4c5",
   "metadata": {},
   "source": [
    "# 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfcc774",
   "metadata": {},
   "source": [
    "De même, lorsque les extraits coïncident on observe un grand pic autour d'une valeur d'offset précise (qui est alors le temps de début de l'extrait choisi par rapport au morceau originel). Lorsqu'ils ne coïncident pas, on observe déjà bien moins de pics, et ceux ci sont aléatoirement répartis tout le long du morceau, sans aucun pic notable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f926ef3",
   "metadata": {},
   "source": [
    "# 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52bcb90",
   "metadata": {},
   "source": [
    "Un critère informatique choisi, qui est sans doute loin d'être le meilleur mais a au moins le mérite d'être simple à mettre en place et relativement efficace, est de comparer tous les morceaux avec l'échantillon testé, et de garder le morceau possédant le plus grand pic de corrélation dans l'histogramme avec l'échantillon, c'est-à-dire le morceau ayant une valeur d'offset apparaisssant le plus parmi tous les morceaux.\n",
    "\n",
    "Ce critère a comme défaut principal que le nombre de code de hash des différents morceaux peut beaucoup varier, et ceux ayant peu de hash vont donc être \"défavorisés\" car auront donc des pics de corrélations moins grands que ceux possédant beaucoup plus de code de hash, qui auront alors des pics mécaniquement plus importants en moyenne. Cela peut devenir ainsi particulièrement gênant si on augmente beaucoup le nombre de morceaux dans la base de donnée, et donc sans doute la variabilité du nombre de code de hash.\n",
    "\n",
    "On pourrait imaginer un critère prenant en compte cette différence, par exemple en faisant un rapport entre nombre de code de hash et taille du pic, mais cela n'a pas eu un franc succès."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68c785",
   "metadata": {},
   "source": [
    "# 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b091db4",
   "metadata": {},
   "source": [
    "Nous avons donc fait varier principalement les valeurs de num_peaks, puis de delta t et delta f pour essayer de déterminer les valeurs fonctionnant le mieux, c'est-à-dire offrant des temps de calcul raisonnables, ainsi qu'un taux de reconnaissance des échantillons décents. Ceux choisis ne sont sans doute pas du tout optimaux encore, mais tatonner à la main est assez fastidieux pour trouver de bonnes valeurs.\n",
    "\n",
    "Ainsi, on a avec le set de valeurs choisies les résultats suivants :\n",
    "\n",
    "En faisant tourner demo.py sur 100 essais, donc 100 tests d'échantillons, le temps moyen d'identification par extrait est de 9.5 secondes, et le taux de reconnaissance est de 92/100 échantillons correctement identifiés.\n",
    "\n",
    "Ces résultats semblent raisonnables à cette échelle, c'est-à-dire en testant avec une base de donnée faite de 9 morceaux. Cependant, en augmentant ce nombre à plusieurs millions comme c'est le cas pour des applications plus connues (Shazam pour ne citer qu'elle), on imagine bien que ce temps de calcul exploserait. Il faudrait alors réflechir à des méthodes plus malignes de comparaison, plutôt que de tout tester naïvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
